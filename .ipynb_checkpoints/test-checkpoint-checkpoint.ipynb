{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input The File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking for file location\n",
    "filepath = input('Enter location of the file: ')\n",
    "\n",
    "#importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Function to load the dataset\n",
    "#def get_dataset(filepath):\n",
    "data = pd.read_csv(filepath, low_memory=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What type of ML Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask them features and target if model is Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input the name of the features and variable\n",
    "x = input(\"Enter the names of features between '' and , after each feature: \")\n",
    "features=[]\n",
    "features=x.split(',')\n",
    "target = input(\"Enter the target between '' : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find the data types of each column\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to count the number of null values in each column\n",
    "count_null=data[data.columns].isna().sum()\n",
    "print(dict(count_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To replace null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=len(data.index)\n",
    "for i in features:\n",
    "    k=count_null[i]\n",
    "    if k!=0:\n",
    "        if k>(rows/2):\n",
    "            data.drop([i], axis = 1)\n",
    "        else:\n",
    "            print('How do u want to replace the null values for feature: ',i)\n",
    "            print('1. with 0\\n2.with mean\\n3.with median\\n4.with mode')\n",
    "            ch=int(input())\n",
    "            if ch==1:\n",
    "                data[i] = data[i].fillna(0)\n",
    "            elif ch==2:\n",
    "                mean=data[i].mean()\n",
    "                data[i] = data[i].fillna(mean)\n",
    "            elif ch==3:\n",
    "                med=data[i].median()\n",
    "                data[i] = data[i].fillna(med)\n",
    "            elif ch==2:\n",
    "                mode=data[i].mode()\n",
    "                data[i] = data[i].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If they don't know features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def featsel():\n",
    "    ax = sns.heatmap(data, annot=True, vmin=0, vmax=1)\n",
    "    print('Check the value printed on each block in the target column.\\n')\n",
    "    print('Value of covariance nearer to 1 better the feature related with target.\\n')\n",
    "    print('After selecting the features input them.\\n\\n\\n')\n",
    "    \n",
    "    #Input the name of the features and variable\n",
    "    features = input(\"Enter the names of features between '' and , after each feature: \")\n",
    "    target = input(\"Enter the target between '' : \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If they know the features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load the features into the X and y respectively\n",
    "def featget(features,target):\n",
    "    X = data.loc[:,[features]]    #Here data is the file that is loaded using filepath\n",
    "    y = data.loc[:,[target]]      #Here data is the file that is loaded using filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Function that returns X after performing oneHotEncoding\n",
    "# columnNumber --> takes the columnNumber for which one hot encoding is to be done\n",
    "def oneHotEncoding(X, columnNumber):\n",
    "    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [columnNumber])], remainder='passthrough')\n",
    "    X = np.array(ct.fit_transform(X))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# X--> takes only a single column\n",
    "def labelEncodingColumn(X):\n",
    "    le = LabelEncoder()\n",
    "    X = le.fit_transform(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binariztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "def binarize(X_train,X_test):\n",
    "    binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "    binary_X = binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test split using sklearn\n",
    "def split_dataset(X, y, testSize, randomState):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # testSize--> defines the size of the test from the dataset(takes decimal less than 1)\n",
    "    # randomState --> takes integer input\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state = randomState)\n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard(X_train,X_test):\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_train,X_test):\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    scaler = Normalizer().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models Start Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearReg(X_train,X_test):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logReg(X_train,X_test):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    logr = LogisticRegression()\n",
    "    logr.fit(X_train,y_train)\n",
    "    y_pred = logr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SV Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc(X_train,X_test):\n",
    "    from sklearn.svm import SVC\n",
    "    svc = SVC(kernel='linear')\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(X_train,X_test,n):\n",
    "    from sklearn import neighbors\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=n)\n",
    "    y_pred = knn.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
